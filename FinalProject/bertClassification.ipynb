{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(r'.\\SpotifyLyricsAnnotated.csv', sep='\\t')\n",
    "\n",
    "# Tokenize the lyrics using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 512\n",
    "\n",
    "def tokenize_lyrics(lyrics):\n",
    "    return tokenizer.encode_plus(\n",
    "        lyrics,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='tf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "subset_data = data.iloc[:2000]\n",
    "\n",
    "subset_data['text'] = subset_data['text'].apply(tokenize_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       "array([[  101,  2298,  2227,  6919,  2227,  2965,  2242,  2569,  2298,\n",
       "         2126,  8451,  5927,  5341,  2028,  3507,  2016,  2015,  2785,\n",
       "         2611,  3084,  2514,  2986,  2071,  2412,  2903,  2071,  3067,\n",
       "         2016,  2015,  2785,  2611,  2302, 10047,  2630,  2412,  3727,\n",
       "         2071,  2071,  2175,  3328,  2380,  4324, 11025,  2015,  2192,\n",
       "         2092,  2175,  3788,  2847,  3331,  2477,  2933,  2016,  2015,\n",
       "         2785,  2611,  3084,  2514,  2986,  2071,  2412,  2903,  2071,\n",
       "         3067,  2016,  2015,  2785,  2611,  2302, 10047,  2630,  2412,\n",
       "         3727,  2071,  2071,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_labels = data['Emotion'].unique()\n",
    "\n",
    "# Create a dictionary to map labels to numbers\n",
    "emotion_mapping = {label: idx for idx, label in enumerate(distinct_labels)}\n",
    "data['Emotion'] = data['Emotion'].map(emotion_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'admiration': 0,\n",
       " 'caring': 1,\n",
       " 'gratitude': 2,\n",
       " 'neutral': 3,\n",
       " 'sadness': 4,\n",
       " 'realization': 5,\n",
       " 'love': 6,\n",
       " 'amusement': 7,\n",
       " 'fear': 8,\n",
       " 'joy': 9,\n",
       " 'anger': 10,\n",
       " 'desire': 11,\n",
       " 'approval': 12,\n",
       " 'excitement': 13,\n",
       " 'surprise': 14,\n",
       " 'confusion': 15,\n",
       " 'annoyance': 16,\n",
       " 'remorse': 17,\n",
       " 'optimism': 18,\n",
       " 'embarrassment': 19,\n",
       " 'curiosity': 20,\n",
       " 'nervousness': 21,\n",
       " 'disgust': 22,\n",
       " 'pride': 23,\n",
       " 'grief': 24,\n",
       " 'disappointment': 25,\n",
       " 'disapproval': 26,\n",
       " 'relief': 27}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Split the data into train and test sets\\ntrain_data, test_data = train_test_split(subset_data, test_size=0.2, random_state=42)\\n\\n# Tokenize the lyrics\\ntrain_lyrics = [tokenize_lyrics(lyrics) for lyrics in train_data['text']]\\ntest_lyrics = [tokenize_lyrics(lyrics) for lyrics in test_data['text']]\\n\\n# Convert emotion labels to numpy arrays\\ntrain_labels = np.array(train_data['Emotion'])\\ntest_labels = np.array(test_data['Emotion'])\\n\\n# Convert the datasets to TensorFlow datasets\\ntrain_dataset = tf.data.Dataset.from_tensor_slices(({\\n    'input_ids': np.concatenate([np.array([lyrics['input_ids']]) for lyrics in train_lyrics]),\\n    'attention_mask': np.concatenate([np.array([lyrics['attention_mask']]) for lyrics in train_lyrics])\\n}, train_labels))\\n\\ntest_dataset = tf.data.Dataset.from_tensor_slices(({\\n    'input_ids': np.concatenate([np.array([lyrics['input_ids']]) for lyrics in test_lyrics]),\\n    'attention_mask': np.concatenate([np.array([lyrics['attention_mask']]) for lyrics in test_lyrics])\\n}, test_labels))\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(subset_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the lyrics\n",
    "train_lyrics = [tokenize_lyrics(lyrics) for lyrics in train_data['text']]\n",
    "test_lyrics = [tokenize_lyrics(lyrics) for lyrics in test_data['text']]\n",
    "\n",
    "# Convert emotion labels to numpy arrays\n",
    "train_labels = np.array(train_data['Emotion'])\n",
    "test_labels = np.array(test_data['Emotion'])\n",
    "\n",
    "# Convert the datasets to TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': np.concatenate([np.array([lyrics['input_ids']]) for lyrics in train_lyrics]),\n",
    "    'attention_mask': np.concatenate([np.array([lyrics['attention_mask']]) for lyrics in train_lyrics])\n",
    "}, train_labels))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "    'input_ids': np.concatenate([np.array([lyrics['input_ids']]) for lyrics in test_lyrics]),\n",
    "    'attention_mask': np.concatenate([np.array([lyrics['attention_mask']]) for lyrics in test_lyrics])\n",
    "}, test_labels))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract input_ids, attention_mask, and labels from tokenized lyrics\n",
    "input_ids = np.array([lyrics['input_ids'] for lyrics in subset_data['text']])\n",
    "attention_mask = np.array([lyrics['attention_mask'] for lyrics in subset_data['text']])\n",
    "labels = np.array(subset_data['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['admiration', 'caring', 'gratitude', ..., 'neutral', 'neutral',\n",
       "       'neutral'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "input_ids_train, input_ids_test, attention_mask_train, attention_mask_test, y_train, y_test = train_test_split(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    subset_data['Emotion'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input_ids_train to remove the extra dimension\n",
    "input_ids_train = input_ids_train.reshape(input_ids_train.shape[0], -1)\n",
    "\n",
    "# Reshape attention_mask_train to remove the extra dimension\n",
    "attention_mask_train = attention_mask_train.reshape(attention_mask_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input_ids_train to remove the extra dimension\n",
    "input_ids_test = input_ids_test.reshape(input_ids_test.shape[0], -1)\n",
    "\n",
    "# Reshape attention_mask_train to remove the extra dimension\n",
    "attention_mask_test = attention_mask_test.reshape(attention_mask_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels to integers\n",
    "y_train_mapped = np.array([emotion_mapping[label] for label in y_train])\n",
    "y_test_mapped = np.array([emotion_mapping[label] for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and batch the training dataset\n",
    "batch_size = 32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids_train, 'attention_mask': attention_mask_train}, y_train_mapped)).shuffle(len(y_train)).batch(batch_size)\n",
    "\n",
    "# Shuffle and batch the test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': input_ids_test, 'attention_mask': attention_mask_test}, y_test_mapped)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  3, 14,  6, 20,  3,  8,  6,  3,  6, 10,  6,  4,  3,  3,  3,  3,\n",
       "       10,  3, 11,  3,  3,  3,  3, 11,  3,  7,  6,  6,  3,  4,  8,  3, 10,\n",
       "        6,  3,  3,  5,  3,  6,  3,  8,  3, 15,  3,  6,  4,  3,  7,  3,  3,\n",
       "        0,  3,  3,  3,  3,  9,  3,  4,  4,  3,  4,  4,  3,  3,  3,  0, 11,\n",
       "        3,  8,  6,  3,  7,  3,  4,  6,  3,  3,  6,  3,  4,  8, 20,  3,  3,\n",
       "        9, 12,  4,  3,  3,  3,  0,  1,  6,  3,  9,  3,  1,  4,  3,  8, 24,\n",
       "       10,  3,  4,  8,  3,  6, 11,  3,  0,  7,  9,  3,  4,  5,  6,  6,  3,\n",
       "        3,  6,  0,  6,  3,  3,  3,  3,  3,  3,  6,  3,  6,  6,  8,  4,  0,\n",
       "       15,  8, 23, 17,  6,  6,  3,  7,  3,  4,  3,  3, 16,  3,  3,  7,  3,\n",
       "        6,  6,  3,  3,  7,  3,  3,  2,  3,  2,  9,  3,  4,  1,  3, 10,  3,\n",
       "        4,  3,  3,  1,  6,  3,  6,  6,  6, 10,  3,  6,  3,  3,  3,  2, 13,\n",
       "        3,  6,  6,  6,  7,  4,  6,  7,  1,  6,  3,  3,  3,  3,  3,  3,  4,\n",
       "        3,  3,  3,  6,  3,  4, 11,  3,  3,  3,  3,  3,  3,  6,  3,  6,  3,\n",
       "        4,  3,  3,  3,  4,  1,  1, 17,  6,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        4,  3,  3,  3,  6, 10,  3,  6,  3,  3,  6,  9, 14,  3, 26,  3,  6,\n",
       "        6, 10,  3,  2,  3,  4,  3,  3,  3,  4,  3,  3,  3,  3,  3,  4,  3,\n",
       "        3,  3, 17,  3,  3,  6,  8,  8,  5, 20, 15,  3,  3,  7,  4,  3,  3,\n",
       "        3,  3,  8,  3, 11, 14,  6,  3,  3, 18,  6, 20,  4, 11,  4,  3,  6,\n",
       "        6,  3,  8,  6,  8,  3,  4,  6,  3,  3,  3,  3,  6,  3,  6,  3,  3,\n",
       "        3,  3,  4,  3,  4,  6,  3,  3,  6,  4, 10,  6,  6, 13,  3,  9,  3,\n",
       "        3,  8,  6,  3,  3,  3,  3,  3,  3, 13, 17,  3,  3,  3,  6,  6,  3,\n",
       "        3,  3,  3,  3, 21,  4,  6,  3,  6,  3, 13,  3,  8,  3,  9, 12,  3,\n",
       "       18,  6,  3,  6,  6,  0,  3,  3,  3,  3,  3,  3,  7,  3,  3,  3,  3,\n",
       "        3,  3,  6,  8,  6,  3,  3,  1,  3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the neural network model\n",
    "max_length = 512\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Use the pooled output\n",
    "output = tf.keras.layers.Dense(8, activation='softmax')(bert_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul' defined at (most recent call last):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n      app.start()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n      self.io_loop.start()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n      user_expressions, allow_stdin,\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-32-fd2016441c1b>\", line 3, in <module>\n      model.fit(train_dataset, epochs=epochs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n      # if the new size is greater than the old one, we extend the current embeddings with a padding until getting new size\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1088, in call\n      outputs = self.bert(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n      # if the new size is greater than the old one, we extend the current embeddings with a padding until getting new size\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 310, in call\n      attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\nNode: 'model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul'\nOOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_103736]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-fd2016441c1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul' defined at (most recent call last):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n      app.start()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n      self.io_loop.start()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n      self._run_once()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n      handle._run()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n      self.run()\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n      user_expressions, allow_stdin,\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n      raw_cell, store_history, silent, shell_futures)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-32-fd2016441c1b>\", line 3, in <module>\n      model.fit(train_dataset, epochs=epochs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n      # if the new size is greater than the old one, we extend the current embeddings with a padding until getting new size\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1088, in call\n      outputs = self.bert(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1061, in run_call_with_unpacked_inputs\n      # if the new size is greater than the old one, we extend the current embeddings with a padding until getting new size\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 310, in call\n      attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\nNode: 'model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul'\nOOM when allocating tensor with shape[32,12,512,512] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node model_3/tf_bert_model_3/bert/encoder/layer_._9/attention/self/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_103736]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 3\n",
    "model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Shuffle and batch the dataset\\nbatch_size = 32\\ntrain_dataset = X_train.shuffle(len(subset_data)).batch(batch_size)\\n\\n# Define the BERT model\\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')\\n\\n# Define the neural network model\\ninput_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\\nattention_mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\\nbert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Use the pooled output\\noutput = tf.keras.layers.Dense(8, activation='softmax')(bert_output)\\n\\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\\n\\n# Compile the model\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n\\n# Train the model\\nepochs = 3\\nmodel.fit(train_dataset, epochs=epochs)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Shuffle and batch the dataset\n",
    "batch_size = 32\n",
    "train_dataset = X_train.shuffle(len(subset_data)).batch(batch_size)\n",
    "\n",
    "# Define the BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the neural network model\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
    "bert_output = bert_model(input_ids, attention_mask=attention_mask)[1]  # Use the pooled output\n",
    "output = tf.keras.layers.Dense(8, activation='softmax')(bert_output)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 3\n",
    "model.fit(train_dataset, epochs=epochs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
