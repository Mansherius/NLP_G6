{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#import neattext.functions as nfx\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Distilbert Tokenizer \n",
    "#model_ckpt = \"distilbert-base-uncased\"\n",
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1996, 3185, 2001, 2025, 2204, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]} 7\n",
      "['[CLS]', 'the', 'movie', 'was', 'not', 'good', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Encode Our Example Text\n",
    "encoded_text = tokenizer(\"The movie was not good\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(encoded_text,len(encoded_text.input_ids))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'.\\emotion_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Splitting the Data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Clean_Text'], data['Emotion'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27845                       Yes  anniversary come  Cheers \n",
       "25699                                        think bloody \n",
       "24688    Peace state fear kind unknown John Buchan Fill...\n",
       "32367    fat bitches complain weight TOLD EAT MCDONALDS...\n",
       "13168    Heb een lolly gekregen met 12 lollys erin en 1...\n",
       "                               ...                        \n",
       "16850                 Damn Im going stuck american time Xx\n",
       "6265                                haha ill tie guys post\n",
       "11284    jump wagon Worse case scenario die Till #alive...\n",
       "860                                           bby plate :(\n",
       "15795                                  friends try hurt me\n",
       "Name: Clean_Text, Length: 27833, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b70e69b267a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(train_texts[0][0])\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(encoded_text,len(encoded_text.input_ids))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r'.\\emotion_dataset_2.csv')\n",
    "df['Clean_Text'] = df['Clean_Text'].fillna('')\n",
    "df = df.head(800)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode text data\n",
    "encoded_data = tokenizer(df['Clean_Text'].tolist(), padding=True, truncation=True, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(800, 46), dtype=int32, numpy=\n",
       "array([[  101,   102,     0, ...,     0,     0,     0],\n",
       "       [  101, 10878,  2552, ...,     0,     0,     0],\n",
       "       [  101,  2126,  2188, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  8837,  2627, ...,     0,     0,     0],\n",
       "       [  101,  8741, 20934, ...,     0,     0,     0],\n",
       "       [  101,  9826,  2360, ...,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(800, 46), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(800, 46), dtype=int32, numpy=\n",
       "array([[1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TensorFlow tensors to numpy arrays\n",
    "X_data = encoded_data['input_ids'].numpy()\n",
    "y_data = df['Emotion'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'joy', 'sadness', 'joy', 'joy', 'fear', 'sadness',\n",
       "       'surprise', 'surprise', 'surprise', 'anger', 'sadness', 'joy',\n",
       "       'fear', 'anger', 'sadness', 'surprise', 'joy', 'joy', 'surprise',\n",
       "       'sadness', 'joy', 'fear', 'anger', 'fear', 'sadness', 'surprise',\n",
       "       'joy', 'sadness', 'joy', 'joy', 'fear', 'fear', 'shame', 'fear',\n",
       "       'joy', 'sadness', 'surprise', 'sadness', 'surprise', 'surprise',\n",
       "       'anger', 'fear', 'sadness', 'anger', 'fear', 'sadness', 'joy',\n",
       "       'anger', 'joy', 'joy', 'fear', 'sadness', 'sadness', 'sadness',\n",
       "       'sadness', 'fear', 'fear', 'sadness', 'sadness', 'surprise',\n",
       "       'neutral', 'joy', 'joy', 'surprise', 'fear', 'fear', 'fear',\n",
       "       'neutral', 'fear', 'joy', 'joy', 'fear', 'joy', 'sadness', 'fear',\n",
       "       'neutral', 'anger', 'fear', 'fear', 'fear', 'joy', 'anger',\n",
       "       'anger', 'joy', 'surprise', 'surprise', 'surprise', 'anger',\n",
       "       'surprise', 'disgust', 'anger', 'joy', 'fear', 'surprise',\n",
       "       'sadness', 'joy', 'anger', 'sadness', 'joy', 'joy', 'surprise',\n",
       "       'joy', 'fear', 'anger', 'joy', 'surprise', 'sadness', 'joy',\n",
       "       'fear', 'joy', 'sadness', 'fear', 'joy', 'neutral', 'sadness',\n",
       "       'fear', 'sadness', 'sadness', 'joy', 'anger', 'anger', 'surprise',\n",
       "       'joy', 'sadness', 'fear', 'anger', 'sadness', 'joy', 'neutral',\n",
       "       'fear', 'sadness', 'anger', 'joy', 'disgust', 'sadness', 'sadness',\n",
       "       'sadness', 'joy', 'joy', 'neutral', 'joy', 'fear', 'anger', 'fear',\n",
       "       'anger', 'joy', 'joy', 'fear', 'surprise', 'joy', 'joy', 'sadness',\n",
       "       'surprise', 'anger', 'sadness', 'fear', 'anger', 'surprise',\n",
       "       'neutral', 'joy', 'neutral', 'sadness', 'neutral', 'anger', 'fear',\n",
       "       'joy', 'anger', 'joy', 'neutral', 'fear', 'fear', 'joy', 'joy',\n",
       "       'fear', 'fear', 'surprise', 'joy', 'sadness', 'surprise', 'joy',\n",
       "       'surprise', 'sadness', 'sadness', 'surprise', 'anger', 'anger',\n",
       "       'fear', 'anger', 'fear', 'joy', 'fear', 'joy', 'anger', 'anger',\n",
       "       'fear', 'neutral', 'disgust', 'joy', 'fear', 'sadness', 'joy',\n",
       "       'sadness', 'anger', 'disgust', 'joy', 'joy', 'sadness', 'surprise',\n",
       "       'sadness', 'joy', 'anger', 'joy', 'joy', 'surprise', 'sadness',\n",
       "       'joy', 'joy', 'fear', 'fear', 'anger', 'fear', 'sadness',\n",
       "       'sadness', 'fear', 'joy', 'sadness', 'neutral', 'joy', 'anger',\n",
       "       'joy', 'disgust', 'anger', 'fear', 'joy', 'anger', 'anger',\n",
       "       'anger', 'surprise', 'fear', 'fear', 'joy', 'joy', 'joy',\n",
       "       'sadness', 'sadness', 'fear', 'joy', 'sadness', 'fear', 'sadness',\n",
       "       'sadness', 'disgust', 'joy', 'joy', 'anger', 'fear', 'joy', 'fear',\n",
       "       'fear', 'sadness', 'sadness', 'sadness', 'joy', 'fear', 'fear',\n",
       "       'disgust', 'sadness', 'sadness', 'neutral', 'sadness', 'anger',\n",
       "       'surprise', 'sadness', 'sadness', 'joy', 'surprise', 'fear',\n",
       "       'sadness', 'fear', 'surprise', 'neutral', 'anger', 'surprise',\n",
       "       'sadness', 'joy', 'fear', 'fear', 'joy', 'anger', 'anger',\n",
       "       'sadness', 'neutral', 'joy', 'fear', 'sadness', 'sadness',\n",
       "       'disgust', 'anger', 'disgust', 'sadness', 'surprise', 'anger',\n",
       "       'sadness', 'joy', 'fear', 'joy', 'sadness', 'sadness', 'joy',\n",
       "       'joy', 'disgust', 'anger', 'fear', 'joy', 'neutral', 'fear',\n",
       "       'fear', 'sadness', 'surprise', 'fear', 'joy', 'anger', 'surprise',\n",
       "       'fear', 'neutral', 'joy', 'surprise', 'fear', 'sadness', 'joy',\n",
       "       'joy', 'surprise', 'fear', 'surprise', 'sadness', 'disgust',\n",
       "       'anger', 'anger', 'anger', 'anger', 'shame', 'fear', 'joy', 'joy',\n",
       "       'fear', 'joy', 'joy', 'surprise', 'sadness', 'sadness', 'sadness',\n",
       "       'joy', 'fear', 'joy', 'sadness', 'sadness', 'anger', 'sadness',\n",
       "       'anger', 'joy', 'anger', 'joy', 'fear', 'joy', 'joy', 'joy', 'joy',\n",
       "       'sadness', 'sadness', 'anger', 'surprise', 'joy', 'joy', 'anger',\n",
       "       'neutral', 'joy', 'sadness', 'surprise', 'sadness', 'anger',\n",
       "       'surprise', 'anger', 'surprise', 'fear', 'neutral', 'anger',\n",
       "       'fear', 'anger', 'sadness', 'neutral', 'surprise', 'neutral',\n",
       "       'neutral', 'anger', 'joy', 'sadness', 'joy', 'joy', 'surprise',\n",
       "       'sadness', 'joy', 'sadness', 'sadness', 'joy', 'sadness', 'joy',\n",
       "       'sadness', 'anger', 'anger', 'anger', 'surprise', 'joy', 'joy',\n",
       "       'joy', 'surprise', 'fear', 'joy', 'fear', 'joy', 'neutral',\n",
       "       'surprise', 'surprise', 'surprise', 'fear', 'sadness', 'surprise',\n",
       "       'joy', 'joy', 'sadness', 'sadness', 'disgust', 'neutral', 'joy',\n",
       "       'joy', 'sadness', 'surprise', 'joy', 'joy', 'sadness', 'surprise',\n",
       "       'sadness', 'fear', 'sadness', 'joy', 'joy', 'surprise', 'anger',\n",
       "       'fear', 'disgust', 'anger', 'joy', 'joy', 'sadness', 'anger',\n",
       "       'sadness', 'sadness', 'joy', 'surprise', 'sadness', 'surprise',\n",
       "       'joy', 'joy', 'surprise', 'joy', 'surprise', 'sadness', 'anger',\n",
       "       'sadness', 'anger', 'neutral', 'anger', 'joy', 'fear', 'anger',\n",
       "       'anger', 'fear', 'disgust', 'joy', 'disgust', 'joy', 'fear',\n",
       "       'fear', 'joy', 'fear', 'sadness', 'sadness', 'joy', 'anger',\n",
       "       'sadness', 'joy', 'joy', 'joy', 'joy', 'sadness', 'sadness',\n",
       "       'disgust', 'surprise', 'fear', 'sadness', 'joy', 'sadness', 'joy',\n",
       "       'surprise', 'fear', 'joy', 'anger', 'joy', 'sadness', 'joy', 'joy',\n",
       "       'joy', 'joy', 'fear', 'fear', 'sadness', 'joy', 'surprise',\n",
       "       'anger', 'fear', 'joy', 'fear', 'anger', 'joy', 'joy', 'joy',\n",
       "       'surprise', 'surprise', 'fear', 'fear', 'disgust', 'sadness',\n",
       "       'disgust', 'fear', 'fear', 'neutral', 'anger', 'surprise',\n",
       "       'sadness', 'neutral', 'neutral', 'joy', 'surprise', 'neutral',\n",
       "       'anger', 'joy', 'fear', 'joy', 'fear', 'neutral', 'joy', 'fear',\n",
       "       'neutral', 'fear', 'sadness', 'sadness', 'anger', 'anger', 'anger',\n",
       "       'anger', 'anger', 'anger', 'sadness', 'joy', 'fear', 'joy',\n",
       "       'sadness', 'sadness', 'joy', 'surprise', 'fear', 'anger', 'joy',\n",
       "       'joy', 'joy', 'joy', 'joy', 'fear', 'anger', 'neutral', 'surprise',\n",
       "       'anger', 'anger', 'joy', 'sadness', 'joy', 'joy', 'joy', 'joy',\n",
       "       'fear', 'sadness', 'joy', 'joy', 'joy', 'anger', 'joy', 'sadness',\n",
       "       'anger', 'surprise', 'fear', 'anger', 'fear', 'joy', 'joy',\n",
       "       'anger', 'sadness', 'sadness', 'fear', 'anger', 'joy', 'shame',\n",
       "       'sadness', 'joy', 'anger', 'anger', 'joy', 'anger', 'fear', 'fear',\n",
       "       'sadness', 'surprise', 'sadness', 'sadness', 'joy', 'sadness',\n",
       "       'neutral', 'joy', 'joy', 'fear', 'anger', 'joy', 'joy', 'anger',\n",
       "       'sadness', 'anger', 'joy', 'joy', 'fear', 'joy', 'joy', 'neutral',\n",
       "       'surprise', 'surprise', 'surprise', 'fear', 'sadness', 'fear',\n",
       "       'joy', 'joy', 'neutral', 'joy', 'neutral', 'joy', 'surprise',\n",
       "       'fear', 'joy', 'surprise', 'fear', 'sadness', 'sadness', 'joy',\n",
       "       'joy', 'fear', 'joy', 'fear', 'sadness', 'neutral', 'fear',\n",
       "       'sadness', 'sadness', 'neutral', 'sadness', 'joy', 'sadness',\n",
       "       'fear', 'joy', 'fear', 'anger', 'sadness', 'fear', 'joy', 'joy',\n",
       "       'fear', 'sadness', 'joy', 'joy', 'joy', 'joy', 'sadness', 'joy',\n",
       "       'sadness', 'sadness', 'surprise', 'fear', 'joy', 'joy', 'sadness',\n",
       "       'surprise', 'neutral', 'surprise', 'anger', 'fear', 'sadness',\n",
       "       'joy', 'anger', 'fear', 'anger', 'fear', 'fear', 'sadness', 'fear',\n",
       "       'surprise', 'anger', 'sadness', 'joy', 'fear', 'fear', 'joy',\n",
       "       'joy', 'sadness', 'surprise', 'sadness', 'joy', 'sadness', 'joy',\n",
       "       'sadness', 'joy', 'anger', 'anger', 'sadness', 'sadness', 'joy',\n",
       "       'sadness', 'anger', 'joy', 'anger', 'fear', 'fear', 'joy', 'joy',\n",
       "       'neutral', 'sadness', 'surprise', 'fear', 'joy', 'surprise',\n",
       "       'shame', 'sadness', 'fear', 'anger', 'joy', 'joy', 'sadness',\n",
       "       'joy', 'joy', 'joy', 'sadness', 'joy', 'sadness', 'fear', 'anger',\n",
       "       'joy', 'sadness', 'sadness', 'joy', 'neutral', 'surprise', 'fear',\n",
       "       'anger', 'anger', 'anger', 'joy', 'joy', 'fear', 'neutral', 'fear',\n",
       "       'anger', 'surprise', 'fear', 'fear', 'sadness', 'sadness',\n",
       "       'surprise', 'sadness', 'sadness', 'sadness', 'surprise', 'sadness',\n",
       "       'disgust', 'neutral', 'sadness', 'anger', 'neutral', 'sadness',\n",
       "       'neutral', 'anger', 'fear', 'joy', 'sadness', 'joy'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to one-hot encoding\n",
    "#num_classes = len(df['Emotion'].unique())\n",
    "#y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = ['accuracy']\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "12/20 [=================>............] - ETA: 2:05 - loss: 1.8998 - accuracy: 0.2604"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = bert_model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = bert_model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Save the model\n",
    "bert_model.save_pretrained('bert_classification_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
