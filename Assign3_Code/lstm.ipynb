{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3326: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "b'Skipping line 163: expected 2 fields, saw 18\\nSkipping line 244: expected 2 fields, saw 74\\nSkipping line 491: expected 2 fields, saw 47\\nSkipping line 873: expected 2 fields, saw 47\\nSkipping line 890: expected 2 fields, saw 29\\nSkipping line 1012: expected 2 fields, saw 52\\nSkipping line 1234: expected 2 fields, saw 16\\nSkipping line 1237: expected 2 fields, saw 8\\nSkipping line 1306: expected 2 fields, saw 49\\nSkipping line 1352: expected 2 fields, saw 21\\nSkipping line 1725: expected 2 fields, saw 56\\nSkipping line 1859: expected 2 fields, saw 5\\nSkipping line 2006: expected 2 fields, saw 32\\nSkipping line 2043: expected 2 fields, saw 24\\nSkipping line 2280: expected 2 fields, saw 28\\nSkipping line 2292: expected 2 fields, saw 29\\nSkipping line 2586: expected 2 fields, saw 20\\nSkipping line 2743: expected 2 fields, saw 4\\nSkipping line 2783: expected 2 fields, saw 14\\nSkipping line 3070: expected 2 fields, saw 22\\nSkipping line 3267: expected 2 fields, saw 23\\nSkipping line 3428: expected 2 fields, saw 68\\nSkipping line 3432: expected 2 fields, saw 32\\nSkipping line 3562: expected 2 fields, saw 36\\nSkipping line 3946: expected 2 fields, saw 52\\nSkipping line 4002: expected 2 fields, saw 27\\nSkipping line 4078: expected 2 fields, saw 14\\nSkipping line 4108: expected 2 fields, saw 19\\nSkipping line 4318: expected 2 fields, saw 6\\nSkipping line 4381: expected 2 fields, saw 41\\nSkipping line 4687: expected 2 fields, saw 7\\nSkipping line 4865: expected 2 fields, saw 41\\nSkipping line 4978: expected 2 fields, saw 42\\nSkipping line 5239: expected 2 fields, saw 27\\nSkipping line 5308: expected 2 fields, saw 54\\nSkipping line 5382: expected 2 fields, saw 3\\nSkipping line 5541: expected 2 fields, saw 58\\nSkipping line 5635: expected 2 fields, saw 7\\nSkipping line 6029: expected 2 fields, saw 40\\nSkipping line 6101: expected 2 fields, saw 35\\nSkipping line 6104: expected 2 fields, saw 4\\nSkipping line 6287: expected 2 fields, saw 56\\nSkipping line 6413: expected 2 fields, saw 65\\nSkipping line 6702: expected 2 fields, saw 7\\nSkipping line 6950: expected 2 fields, saw 36\\nSkipping line 7033: expected 2 fields, saw 5\\nSkipping line 7320: expected 2 fields, saw 28\\nSkipping line 8139: expected 2 fields, saw 4\\nSkipping line 8155: expected 2 fields, saw 44\\nSkipping line 8289: expected 2 fields, saw 10\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
      "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
      "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
      "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
      "4  ENTJ  'You're fired.|||That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('.\\mbti_1.csv', encoding='latin1', error_bad_lines=False)  # Replace 'path_to_your_dataset.csv' with the actual path\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                                              posts\n",
      "0  INFJ  ['http://www.youtube.com/watch?v=qsXHcwe3krw, ...\n",
      "1  ENTP  ['I'm finding the lack of me in these posts ve...\n",
      "2  INTP  ['Good one  _____   https://www.youtube.com/wa...\n",
      "3  INTJ  ['Dear INTP,   I enjoyed our conversation the ...\n",
      "4  ENTJ  ['You're fired., That's another silly misconce...\n"
     ]
    }
   ],
   "source": [
    "# Splitting text posts into individual posts\n",
    "data['posts'] = data['posts'].apply(lambda x: x.split('|||'))\n",
    "\n",
    "# Display the updated dataset with split posts\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Remove commas from text posts using regular expressions\n",
    "#data['posts'] = data['posts'].apply(lambda x: re.sub(r',', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized and Padded Sequences (Train):\n",
      "[[1534 1535  443 1536 1537  444 1538 1539 1540 1541 1542 1543 1544 1545\n",
      "  1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559\n",
      "  1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573\n",
      "  1574 1575 1576 1577 1578 1579 1580 1581    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1582 1583 1584 1585 1586 1587 1588 1589 1590 1591  445 1592 1593 1594\n",
      "  1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608\n",
      "  1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622\n",
      "  1623 1624 1625 1626 1627 1628 1629 1630    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644\n",
      "  1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658\n",
      "  1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672\n",
      "  1673 1674 1675  108 1676 1677 1678 1679    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1680 1681 1682 1683 1684 1685   88 1686 1687 1688 1689 1690 1691 1692\n",
      "  1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706\n",
      "  1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720\n",
      "  1721 1722 1723 1724 1725 1726 1727 1728    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742\n",
      "  1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756\n",
      "  1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770\n",
      "  1771 1772 1773 1774 1775 1776 1777 1778    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "Shape of Train Sequences Padded: (6900, 100)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['posts'])\n",
    "\n",
    "max_len = 100  # Adjust max length as needed\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['posts'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['posts'])\n",
    "\n",
    "train_sequences_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Display some preprocessed data\n",
    "print(\"Tokenized and Padded Sequences (Train):\")\n",
    "print(train_sequences_padded[:5])\n",
    "print(\"Shape of Train Sequences Padded:\", train_sequences_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Train Labels: [ 9  0  9 10 11]\n"
     ]
    }
   ],
   "source": [
    "# Encode MBTI type labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['type'])\n",
    "\n",
    "train_labels = label_encoder.transform(train_data['type'])\n",
    "test_labels = label_encoder.transform(test_data['type'])\n",
    "\n",
    "# Display encoded labels\n",
    "print(\"Encoded Train Labels:\", train_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(16, activation='softmax'))  # 16 output classes (MBTI types)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/173 [==============================] - 112s 621ms/step - loss: 2.3402 - accuracy: 0.1911 - val_loss: 2.2530 - val_accuracy: 0.2232\n",
      "Epoch 2/10\n",
      "173/173 [==============================] - 116s 672ms/step - loss: 2.2989 - accuracy: 0.1978 - val_loss: 2.2509 - val_accuracy: 0.2232\n",
      "Epoch 3/10\n",
      "173/173 [==============================] - 118s 681ms/step - loss: 2.2971 - accuracy: 0.1989 - val_loss: 2.2474 - val_accuracy: 0.2232\n",
      "Epoch 4/10\n",
      "173/173 [==============================] - 117s 677ms/step - loss: 2.2972 - accuracy: 0.1951 - val_loss: 2.2479 - val_accuracy: 0.2232\n",
      "Epoch 5/10\n",
      "173/173 [==============================] - 119s 687ms/step - loss: 2.2967 - accuracy: 0.1962 - val_loss: 2.2490 - val_accuracy: 0.2232\n",
      "Epoch 6/10\n",
      "173/173 [==============================] - 117s 674ms/step - loss: 2.2938 - accuracy: 0.1986 - val_loss: 2.2510 - val_accuracy: 0.2232\n",
      "Epoch 7/10\n",
      "173/173 [==============================] - 117s 678ms/step - loss: 2.2940 - accuracy: 0.2031 - val_loss: 2.2449 - val_accuracy: 0.2232\n",
      "Epoch 8/10\n",
      "173/173 [==============================] - 118s 680ms/step - loss: 2.2942 - accuracy: 0.1991 - val_loss: 2.2509 - val_accuracy: 0.2232\n",
      "Epoch 9/10\n",
      "173/173 [==============================] - 117s 677ms/step - loss: 2.2929 - accuracy: 0.2007 - val_loss: 2.2478 - val_accuracy: 0.2232\n",
      "Epoch 10/10\n",
      "173/173 [==============================] - 125s 725ms/step - loss: 2.2944 - accuracy: 0.1991 - val_loss: 2.2509 - val_accuracy: 0.2232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cb072879c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sequences_padded, train_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 29ms/step - loss: 2.2778 - accuracy: 0.2278\n",
      "Test Loss: 2.2777645587921143\n",
      "Test Accuracy: 0.2278260886669159\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_sequences_padded, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 2s 29ms/step\n",
      "Accuracy: 0.2278\n",
      "Precision: 0.0519\n",
      "Recall: 0.2278\n",
      "F1-score: 0.0845\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0   0   0   0   0   0  33   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 114   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  36   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 126   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  10   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   6   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  21   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 262   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 393   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 258   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 244   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  39   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  61   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  43   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  71   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manya\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for each class\n",
    "y_pred_probabilities = model.predict(test_sequences_padded)\n",
    "\n",
    "# Convert probabilities to class labels (argmax to get the index of the highest probability)\n",
    "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(test_labels, y_pred, average='weighted')\n",
    "recall = recall_score(test_labels, y_pred, average='weighted')\n",
    "f1 = f1_score(test_labels, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(test_labels, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize StratifiedKFold for classification\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define number of folds\n",
    "n_splits = 5\n",
    "\n",
    "# Initialize StratifiedKFold for classification\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for train_idx, val_idx in skf.split(data['posts'], data['type']):\n",
    "    # Split data into train and validation sets based on fold indices\n",
    "    X_train, X_val = data['posts'][train_idx], data['posts'][val_idx]\n",
    "    y_train, y_val = data['type'][train_idx], data['type'][val_idx]\n",
    "\n",
    "    # Encode MBTI type labels to numeric labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "    # Tokenization and Padding\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    max_len = 100  # Adjust max length as needed\n",
    "\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_val_padded = pad_sequences(X_val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Define and compile LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_len))\n",
    "    model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer with number of classes\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train LSTM model\n",
    "    model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate LSTM model on validation data\n",
    "    y_pred_encoded = np.argmax(model.predict(X_val_padded), axis=1)\n",
    "\n",
    "    # Decode predicted labels back to MBTI types\n",
    "    y_pred_decoded = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    # Calculate evaluation metrics for this fold\n",
    "    accuracy = accuracy_score(y_val, y_pred_decoded)\n",
    "    precision = precision_score(y_val, y_pred_decoded, average='weighted')\n",
    "    recall = recall_score(y_val, y_pred_decoded, average='weighted')\n",
    "    f1 = f1_score(y_val, y_pred_decoded, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_val, y_pred_decoded)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    confusion_matrices.append(conf_matrix)\n",
    "\n",
    "    # Print or display metrics for this fold\n",
    "    print(f\"Fold Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "# Calculate average metrics across all folds\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_precision = np.mean(precisions)\n",
    "avg_recall = np.mean(recalls)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Print or display average metrics across all folds\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1-score: {avg_f1:.4f}\")\n",
    "\n",
    "# Display average confusion matrix\n",
    "avg_conf_matrix = np.mean(confusion_matrices, axis=0)\n",
    "print(\"Average Confusion Matrix:\")\n",
    "print(avg_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
